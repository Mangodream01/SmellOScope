{"cells":[{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport random\nfrom PIL import Image\n\nimport keras\nimport tensorflow\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n\nimport os\nimport time\nfrom IPython import display\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import class_weight\n\nimport itertools\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nos.listdir('../input')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create the directory structure\nIn these folders we will store the images that will later be fed to the Keras generators."},{"metadata":{"trusted":true},"cell_type":"code","source":"# Create a new directory\nbase_dir = 'base_dir'\nos.mkdir(base_dir)\n\n\n#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n\n# now we create 7 folders inside 'base_dir':\n\n# train_dir\n    # Meaty\n    # Sweet\n    # Piquant\n    # Sour\n    # Salty\n    # Bitter\n \n# val_dir\n    # Meaty\n    # Sweet\n    # Piquant\n    # Sour\n    # Salty\n    # Bitter\n\n# create a path to 'base_dir' to which we will join the names of the new folders\n# train_dir\ntrain_dir = os.path.join(base_dir, 'train_dir')\nos.mkdir(train_dir)\n\n# val_dir\nval_dir = os.path.join(base_dir, 'val_dir')\nos.mkdir(val_dir)\n\n\n# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n# Inside each folder we create seperate folders for each class\n\n# create new folders inside train_dir\nmeaty = os.path.join(train_dir, 'Meaty')\nos.mkdir(meaty)\nsweet = os.path.join(train_dir, 'Sweet')\nos.mkdir(sweet)\npiq = os.path.join(train_dir, 'Piquant')\nos.mkdir(piq)\nsour = os.path.join(train_dir, 'Sour')\nos.mkdir(sour)\nsalty = os.path.join(train_dir, 'Salty')\nos.mkdir(salty)\nbitter = os.path.join(train_dir, 'Bitter')\nos.mkdir(bitter)\n\n\n\n# create new folders inside val_dir\nmeaty = os.path.join(val_dir, 'Meaty')\nos.mkdir(meaty)\nsweet = os.path.join(val_dir, 'Sweet')\nos.mkdir(sweet)\npiq = os.path.join(val_dir, 'Piquant')\nos.mkdir(piq)\nsour = os.path.join(val_dir, 'Sour')\nos.mkdir(sour)\nsalty = os.path.join(val_dir, 'Salty')\nos.mkdir(salty)\nbitter = os.path.join(val_dir, 'Bitter')\nos.mkdir(bitter)\n","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create Train and Val Sets"},{"metadata":{"trusted":true},"cell_type":"code","source":"# read metadata\n\ndf_meta = pd.read_csv('../input/metadata27368.csv')\n#df_meta.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"df_flav = pd.read_csv('../input/flavours.csv')\nprint(df_flav.columns)\nflav = df_flav.values\nprint(flav)\n# rearrange columns in alphabetical order of flavours (to match those used in data augmentation)\nyour_permutation = [1,5,2,4,3,0]\ni = np.argsort(your_permutation)\nflav = flav[:,i]\nprint(flav)\nencoded_flav = np.argmax(flav, axis=1)\nprint(encoded_flav)\ndf_flav['y'] = encoded_flav","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# import faulty images indexes\n\nfile = open(\"../input/faulty_images.txt\", \"r\")\nfaul_id = []\nfor val in file.read().split():\n    faul_id.append(int(val)-1)\nfile.close()\nprint(len(faul_id),\"faulty images\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# get the unlabeled images indexes\n\n# search for nan values in flavours df\nnan_bool = df_flav.isnull().any(axis=1)\nnan_id = np.where(nan_bool == True)[0]\nprint(len(nan_id),'unlabeled images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# remove faulty images from unlabeles\n\nf_nan_id = []\nfor i in faul_id:\n    t = np.where(nan_id == i)[0]\n    if len(t) != 0:\n        f_nan_id.append(t)\nnan_id = np.delete(nan_id, f_nan_id)\n\nprint(len(nan_id),'true unlabeled images')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Good images !\n\ngood_id = np.arange(27638)\ngood_id = np.delete(good_id, np.union1d(faul_id, nan_id))\n\nprint(len(good_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(df_flav['y'].iloc[good_id].value_counts())\n\ny = df_flav['y'].iloc[good_id]\n\nplt.hist(y)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\n# Train and validation split \ntrain_id, val_id, _, _ = train_test_split(good_id, good_id, test_size=0.2, random_state=42, stratify=y)\n\nprint('Train', len(train_id))\nprint('Val',len(val_id))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train and val distibution\n\nplt.subplot(121)\nplt.hist(df_flav['y'].iloc[train_id])\nplt.title('train')\nplt.subplot(122)\nplt.hist(df_flav['y'].iloc[val_id])\nplt.title('validation')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Transfer the Images into the Folders\n\n### Bitter --> 0, Meaty --> 1, Piquant --> 2, Salty --> 3, Sour --> 4, Sweet --> 5"},{"metadata":{"trusted":true},"cell_type":"code","source":"def y2label(y):\n    if y == 0:\n        label = 'Bitter'\n    elif y == 1:\n        label = 'Meaty'\n    elif y == 2:\n        label = 'Piquant'\n    elif y == 3:\n        label = 'Salty'\n    elif y == 4:\n        label = 'Sour'\n    else:\n        label = 'Sweet'\n    return label\n\ndef label2y(label):\n    if label == 'Bitter':\n        y = 0\n    elif label == 'Meaty':\n        y = 1\n    elif label == 'Piquant':\n        y = 2\n    elif label == 'Salty':\n        y = 3\n    elif label == 'Sour':\n        y = 4\n    else:\n        y = 5\n    return y\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# image folder\nimg_folder = \"../input/images27638/images27638/\"\n\n# images indexes (name): train_id, val_id\n\n\n# Transfer the train images\nfor i in train_id:\n    im = str(i+1)\n    while len(im) < 5 :\n        im = '0' + im\n    im ='img' + im + '.jpg'\n    fname = im\n    \n    y_i = df_flav['y'].iloc[i]\n    label = y2label(y_i)\n    \n    # source path to image\n    src = os.path.join(img_folder, fname)\n    # destination path to image\n    dst = os.path.join(train_dir, label, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n\n# Transfer the val images\nfor i in val_id:\n    im = str(i+1)\n    while len(im) < 5 :\n        im = '0' + im\n    im ='img' + im + '.jpg'\n    fname = im\n    \n    y_i = df_flav['y'].iloc[i]\n    label = y2label(y_i)\n    \n    # source path to image\n    src = os.path.join(img_folder, fname)\n    # destination path to image\n    dst = os.path.join(val_dir, label, fname)\n    # copy the image from the source to the destination\n    shutil.copyfile(src, dst)\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many train images we have in each folder\n\nprint(len(os.listdir('base_dir/train_dir/Meaty/')))\nprint(len(os.listdir('base_dir/train_dir/Sweet/')))\nprint(len(os.listdir('base_dir/train_dir/Piquant/')))\nprint(len(os.listdir('base_dir/train_dir/Sour/')))\nprint(len(os.listdir('base_dir/train_dir/Salty/')))\nprint(len(os.listdir('base_dir/train_dir/Bitter/')))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# check how many val images we have in each folder\n\nprint(len(os.listdir('base_dir/val_dir/Meaty/')))\nprint(len(os.listdir('base_dir/val_dir/Sweet/')))\nprint(len(os.listdir('base_dir/val_dir/Piquant/')))\nprint(len(os.listdir('base_dir/val_dir/Sour/')))\nprint(len(os.listdir('base_dir/val_dir/Salty/')))\nprint(len(os.listdir('base_dir/val_dir/Bitter/')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Copy the train images  into aug_dir"},{"metadata":{"trusted":true},"cell_type":"code","source":"# note that we are not augmenting class 'Bitter'\nclass_list = ['Meaty','Sweet','Piquant','Salty','Sour']\n\nfor item in class_list:\n    \n    # We are creating temporary directories here because we delete these directories later\n    # create a base dir\n    aug_dir = 'aug_dir'\n    os.mkdir(aug_dir)\n    # create a dir within the base dir to store images of the same class\n    img_dir = os.path.join(aug_dir, 'img_dir')\n    os.mkdir(img_dir)\n\n    # Choose a class\n    img_class = item\n\n    # list all images in that directory\n    img_list = os.listdir('base_dir/train_dir/' + img_class)\n\n    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n    for fname in img_list:\n            # source path to image\n            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n            # destination path to image\n            dst = os.path.join(img_dir, fname)\n            # copy the image from the source to the destination\n            shutil.copyfile(src, dst)\n\n\n    # point to a dir containing the images and not to the images themselves\n    path = aug_dir\n    save_path = 'base_dir/train_dir/' + img_class\n\n    # Create a data generator\n    datagen = ImageDataGenerator(\n        rotation_range=90,\n        width_shift_range=0.1,\n        height_shift_range=0.1,\n        zoom_range=0.1,\n        horizontal_flip=True,\n        vertical_flip=True,\n        brightness_range=(0.9,1.1),\n        fill_mode='nearest')\n\n    batch_size = 50\n\n    aug_datagen = datagen.flow_from_directory(path,\n                                           save_to_dir=save_path,\n                                           save_format='jpg',\n                                                    target_size=(224,224),\n                                                    batch_size=batch_size)\n\n\n\n    # Generate the augmented images and add them to the training folders\n    \n    ###########\n    \n    num_aug_images_wanted = 4600 # total number of images we want to have in each class\n    \n    ###########\n    \n    num_files = len(os.listdir(img_dir))\n    num_batches = int(np.ceil((num_aug_images_wanted-num_files)/batch_size))\n\n    # run the generator and create about 6000 augmented images\n    for i in range(0,num_batches):\n\n        imgs, labels = next(aug_datagen)\n        \n    # delete temporary directory with the raw image files\n    shutil.rmtree('aug_dir')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Check how many train images we now have in each folder.\n# This is the original images plus the augmented images.\n\nprint(len(os.listdir('base_dir/train_dir/Meaty/')))\nprint(len(os.listdir('base_dir/train_dir/Sweet/')))\nprint(len(os.listdir('base_dir/train_dir/Piquant/')))\nprint(len(os.listdir('base_dir/train_dir/Sour/')))\nprint(len(os.listdir('base_dir/train_dir/Salty/')))\nprint(len(os.listdir('base_dir/train_dir/Bitter/')))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Set Up the Generators"},{"metadata":{"trusted":true},"cell_type":"code","source":"train_path = 'base_dir/train_dir'\nvalid_path = 'base_dir/val_dir'\n\nnum_train_samples = len(train_id)\nnum_val_samples = len(val_id)\ntrain_batch_size = 10\nval_batch_size = 10\nimage_size = 224\n\ntrain_steps = np.ceil(num_train_samples / train_batch_size)\nval_steps = np.ceil(num_val_samples / val_batch_size)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\ndatagen = ImageDataGenerator(\n    preprocessing_function= \\\n    tensorflow.keras.applications.mobilenet.preprocess_input)\n\ntrain_batches = datagen.flow_from_directory(train_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=train_batch_size)\n\nvalid_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=val_batch_size)\n\n# Note: shuffle=False causes the test dataset to not be shuffled\ntest_batches = datagen.flow_from_directory(valid_path,\n                                            target_size=(image_size,image_size),\n                                            batch_size=1,\n                                            shuffle=False)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Modify MobileNet Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"# create a copy of a mobilenet model\n\nmobile = tensorflow.keras.applications.mobilenet.MobileNet()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# CREATE THE MODEL ARCHITECTURE\n\n# Exclude the last 5 layers of the above model.\n# This will include all layers up to and including global_average_pooling2d_1\nx = mobile.layers[-6].output\n\n# Create a new dense layer for predictions\n# 7 corresponds to the number of classes\nx = Dropout(0.25)(x)\npredictions = Dense(6, activation='softmax')(x)\n\n# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n# dense layer we created above.\n\nmodel = Model(inputs=mobile.input, outputs=predictions)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need to choose how many layers we actually want to be trained.\n\n# Here we are freezing the weights of all layers except the\n# last 23 layers in the new model.\n# The last 23 layers of the model will be trained.\n\nfor layer in model.layers[:-23]:\n    layer.trainable = False","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Train the Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n              metrics=['acc'])\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nfilepath = \"../working/model.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=True, mode='max')\n\nreduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n                                   verbose=1, mode='max', min_lr=0.00001)\n                              \n    \ncallbacks_list = [checkpoint, reduce_lr]\n\nhistory = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n                    validation_data=valid_batches,\n                    validation_steps=val_steps,\n                    epochs=6, verbose=1,\n                   callbacks=callbacks_list)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Best model\nmodel.load_weights(filepath)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Plot the Training Curves"},{"metadata":{"trusted":true},"cell_type":"code","source":"# display the loss and accuracy curves\n\nimport matplotlib.pyplot as plt\n\nacc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss, 'bo', label='Training loss')\nplt.plot(epochs, val_loss, 'b', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.figure()\nplt.show()\n\nprint('Best validation accuracy: ', np.max(val_acc))\nprint('On epoch: ', epochs[np.where(val_acc == np.max(val_acc))[0][0]])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Create a Confusion Matrix"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import confusion_matrix\n\n# Get the labels of the test images.\n\ntest_labels = test_batches.classes","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# We need these to plot the confusion matrix.\ntest_labels","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Print the label associated with each class\ntest_batches.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# make a prediction\npredictions = model.predict_generator(test_batches, steps=len(val_id), verbose=1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictions.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Source: Scikit Learn website\n# http://scikit-learn.org/stable/auto_examples/\n# model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n# selection-plot-confusion-matrix-py\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() / 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    plt.tight_layout()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_labels.shape","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# argmax returns the index of the max value in a row\ncm = confusion_matrix(test_labels, predictions.argmax(axis=1))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_batches.class_indices","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Define the labels of the class indices. These need to match the \n# order shown above.\ncm_plot_labels = ['Bitter', 'Meaty', 'Piquant', 'Salty', 'Sour','Sweet']\n\nplot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# delete temporary directory with the raw image files\nshutil.rmtree('base_dir')","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Test on images"},{"metadata":{"trusted":true},"cell_type":"code","source":"# SAVE ENCODED LAYERS FOR LATER VISUALIZATIONS\n# Input the model and a layer to encode, which can be visualized later\ndef encoder(model,layer):\n    selection = 'conv_pw_' + str(layer)\n    encoder = Model(inputs=model.input,outputs=model.get_layer(selection).output)\n    #encoder.summary()\n    return(encoder)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def plot_pie_chart(model, img, labels):\n    #fig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=\"equal\"))\n\n    recipe = labels\n\n    data = model.predict(img)[0]\n\n    wedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)\n\n    bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n    kw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n              bbox=bbox_props, zorder=0, va=\"center\")\n\n    for i, p in enumerate(wedges):\n        ang = (p.theta2 - p.theta1)/2. + p.theta1\n        y = np.sin(np.deg2rad(ang))\n        x = np.cos(np.deg2rad(ang))\n        horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n        connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n        kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n        ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n                     horizontalalignment=horizontalalignment, **kw)\n\n    ax.set_title(\"What AI sees\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# creat a useful dictionary structures\n\npartition = {}\npartition['train'] = train_id\npartition['validation'] = val_id\npartition['faulty'] = faul_id\npartition['no_label'] = nan_id\n\nlabels = {}\nname = {}\nimage = {}\nfor i in range(27638):\n    \n    labels[str(i)] = df_flav.iloc[[i]].values[0] # every label is a panda df rows\n    \n    name[str(i)] = df_meta['name'][i]\n    \n    im = str(i+1)\n    while len(im) < 5 :\n        im = '0' + im\n    im ='img' + im + '.jpg'\n    image[str(i)] = im","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"num_images = 50\nnp.random.seed(42)\nrandom_images = random.sample(list(nan_id),num_images)\n\nencoded = encoder(model, 12)\n\n\n\nfor i, img_id in enumerate(random_images):\n    \n    plt.figure(figsize=(12,12))\n    #dsaodskaod\n    \n    #Load in example image\n    im = Image.open(\"../input/images27638/images27638/\"+image[str(img_id)])\n    im = im.resize((224, 224), Image.ANTIALIAS)\n    im.load()\n    im = np.asarray(im, dtype=np.uint8)/255\n\n    #Plot original image\n    ax = plt.subplot(1, 3, 1)\n    plt.imshow(im)\n    plt.title('Original')\n    im = np.expand_dims(im, axis=0)\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n    \n    #[LORENZO] HERE WE WANT TO VISUALIZE THE ENCODED LAYERS\n    #Plot encoded image (\"what the AE sees\")\n    ax = plt.subplot(1, 3, 2)\n    encoded_img = encoded.predict(im)\n    #This is still from the autoencoder so we have to reshape differently\n    #To plot the image we need to know the dimension of that layer\n    #This can be found by multiplication e.g. \n    #conv_pw_12 (Conv2D)  (None, 7, 7, 1024)    = 7*7*1024 = 50,176 dimensions\n    #We need to reshape the image to fit this dimension e.g. 256 x 196 = 50,176 \n    plt.imshow(encoded_img.reshape(256, 196).T)\n    plt.title('Encoded')\n    ax.get_xaxis().set_visible(False)\n    ax.get_yaxis().set_visible(False)\n\n    #Plot decoded image (\"what the AE reconstructed\")\n    ax = plt.subplot(1, 3, 3)\n    plot_pie_chart(model, im, df_flav.columns)\n    plt.show()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Visualize layers other approach"},{"metadata":{"trusted":true},"cell_type":"code","source":"def normalize(x):\n    \"\"\"utility function to normalize a tensor.\n    # Arguments\n        x: An input tensor.\n    # Returns\n        The normalized input tensor.\n    \"\"\"\n    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n\n\ndef deprocess_image(x):\n    \"\"\"utility function to convert a float array into a valid uint8 image.\n    # Arguments\n        x: A numpy-array representing the generated image.\n    # Returns\n        A processed numpy-array, which could be used in e.g. imshow.\n    \"\"\"\n    # normalize tensor: center on 0., ensure std is 0.25\n    x -= x.mean()\n    x /= (x.std() + K.epsilon())\n    x *= 0.25\n\n    # clip to [0, 1]\n    x += 0.5\n    x = np.clip(x, 0, 1)\n\n    # convert to RGB array\n    x *= 255\n    if K.image_data_format() == 'channels_first':\n        x = x.transpose((1, 2, 0))\n    x = np.clip(x, 0, 255).astype('uint8')\n    return x\n\n\ndef process_image(x, former):\n    \"\"\"utility function to convert a valid uint8 image back into a float array.\n       Reverses `deprocess_image`.\n    # Arguments\n        x: A numpy-array, which could be used in e.g. imshow.\n        former: The former numpy-array.\n                Need to determine the former mean and variance.\n    # Returns\n        A processed numpy-array representing the generated image.\n    \"\"\"\n    if K.image_data_format() == 'channels_first':\n        x = x.transpose((2, 0, 1))\n    return (x / 255 - 0.5) * 4 * former.std() + former.mean()\n\n\ndef visualize_layer(model,\n                    input_img_arr,\n                    layer_id,\n                    filter_id,\n                    step=1.,\n                    epochs=15,\n                    upscaling_steps=9,\n                    upscaling_factor=1.2,\n                    output_dim=(224, 224)):\n    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n    # Arguments\n        model: The model containing layer_name.\n        layer_name: The name of the layer to be visualized.\n                    Has to be a part of model.\n        step: step size for gradient ascent.\n        epochs: Number of iterations for gradient ascent.\n        upscaling_steps: Number of upscaling steps.\n                         Starting image is in this case (80, 80).\n        upscaling_factor: Factor to which to slowly upgrade\n                          the image towards output_dim.\n        output_dim: [img_width, img_height] The output image dimensions.\n        filter_range: Tupel[lower, upper]\n                      Determines the to be computed filter numbers.\n                      If the second value is `None`,\n                      the last filter will be inferred as the upper boundary.\n    \"\"\"\n\n    def _generate_filter_image(input_img,\n                              input_im_array,\n                              layer_output,\n                              filter_index):\n        \"\"\"Generates image for one particular filter.\n        # Arguments\n            input_img: The input-image Tensor.\n            layer_output: The output-image Tensor.\n            filter_index: The to be processed filter number.\n                          Assumed to be valid.\n        #Returns\n            Either None if no image could be generated.\n            or a tuple of the image (array) itself and the last loss.\n        \"\"\"\n        s_time = time.time()\n\n        # we build a loss function that maximizes the activation\n        # of the nth filter of the layer considered\n        if K.image_data_format() == 'channels_first':\n            loss = K.mean(layer_output[:, filter_index, :, :])\n        else:\n            loss = K.mean(layer_output[:, :, :, filter_index])\n\n        # we compute the gradient of the input picture wrt this loss\n        grads = K.gradients(loss, input_img)[0]\n\n        # normalization trick: we normalize the gradient\n        grads = normalize(grads)\n\n        # this function returns the loss and grads given the input picture\n        iterate = K.function([input_img], [loss, grads])\n\n        # we start from a gray image with some random noise\n        intermediate_dim = tuple(\n            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n        if K.image_data_format() == 'channels_first':\n            input_img_data = np.expand_dims(np.moveaxis(input_im_array, 2, 0), axis=0)\n        else:\n            input_img_data = np.expand_dims(input_im_array, axis=0)\n            \n        # Slowly upscaling towards the original size prevents\n        # a dominating high-frequency of the to visualized structure\n        # as it would occur if we directly compute the 412d-image.\n        # Behaves as a better starting point for each following dimension\n        # and therefore avoids poor local minima\n        for up in reversed(range(upscaling_steps)):\n            # we run gradient ascent for e.g. 20 steps\n            for _ in range(epochs):\n                loss_value, grads_value = iterate([input_img_data])\n                input_img_data = input_img_data + (grads_value * step)\n\n                # some filters get stuck to 0, we can skip them\n                if loss_value <= K.epsilon():\n                    return None\n\n            # Calulate upscaled dimension\n            intermediate_dim = tuple(\n                int(x / (upscaling_factor ** up)) for x in output_dim)\n            # Upscale\n            img = deprocess_image(input_img_data[0])\n            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n                                                           pil_image.BICUBIC))\n            input_img_data = [process_image(img, input_img_data[0])]\n\n        # decode the resulting input image\n        img = deprocess_image(input_img_data[0])\n        e_time = time.time()\n        #print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n                                                                  #loss_value,\n                                                                  #e_time - s_time))\n        return img, loss_value\n\n    def _draw_filters(filters, n=None):\n        \"\"\"Draw the best filters in a nxn grid.\n        # Arguments\n            filters: A List of generated images and their corresponding losses\n                     for each processed filter.\n            n: dimension of the grid.\n               If none, the largest possible square will be used\n        \"\"\"\n        if n is None:\n            n = int(np.floor(np.sqrt(len(filters))))\n\n        # the filters that have the highest loss are assumed to be better-looking.\n        # we will only keep the top n*n filters.\n        filters.sort(key=lambda x: x[1], reverse=True)\n        filters = filters[:n * n]\n\n        # build a black picture with enough space for\n        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n        MARGIN = 5\n        width = n * output_dim[0] + (n - 1) * MARGIN\n        height = n * output_dim[1] + (n - 1) * MARGIN\n        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n\n        # fill the picture with our saved filters\n        for i in range(n):\n            for j in range(n):\n                img, _ = filters[i * n + j]\n                width_margin = (output_dim[0] + MARGIN) * i\n                height_margin = (output_dim[1] + MARGIN) * j\n                stitched_filters[\n                    width_margin: width_margin + output_dim[0],\n                    height_margin: height_margin + output_dim[1], :] = img\n\n        # save the result to disk\n        #save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n        \n        plt.imshow(stitched_filters)\n        plt.show()\n\n    # this is the placeholder for the input images\n    assert len(model.inputs) == 1\n    input_img = model.inputs[0]\n\n    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n    \n    # get the name of the desired layer\n    for i,layer in enumerate(model.layers):\n        if i == layer_id:\n            output_layer = layer_dict[layer.name]\n\n    \n    assert isinstance(output_layer, layers.Conv2D)\n\n    processed_filters = []\n    \n    img_loss = _generate_filter_image(input_img, input_img_arr, output_layer.output, filter_id)\n\n    if img_loss is not None:\n        processed_filters.append(img_loss)\n\n    print('{} filter processed.'.format(len(processed_filters)))\n    # Finally draw and store the best filters to disk\n    _draw_filters(processed_filters)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from tensorflow.keras.models import load_model\nmodel.save('mobilenet_yummly.h5')\n#del model\nmodel_saved = load_model('mobilenet_yummly.h5')\n\nmodel_saved.summary()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#def prepare_visualization(image = \"../input/images27638/images27638/img01993.jpg\"):\nim = Image.open(image)\nim = im.resize((224,224), Image.ANTIALIAS)\nim.load()\n    \nloaded_img = np.asarray(im, dtype=np.uint8)\n    \n    plt.imshow(loaded_img)\n    plt.show()\n    \n    return loaded_img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_img = \"../input/images27638/images27638/img01982.jpg\"\nvisualize_layer(model_saved, prepare_visualization(),10,10 )","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.3"}},"nbformat":4,"nbformat_minor":1}