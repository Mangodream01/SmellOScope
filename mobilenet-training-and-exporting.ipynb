{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['.ipynb_checkpoints',\n",
       " 'Audacity',\n",
       " 'base_dir',\n",
       " 'data_records_27638.txt',\n",
       " 'desktop.ini',\n",
       " 'faulty_images.txt',\n",
       " 'final-dataset-yummly.zip',\n",
       " 'flavours.csv',\n",
       " 'images27638',\n",
       " 'images27638.zip',\n",
       " 'metadata27368.csv',\n",
       " 'mobilenet-training-and-exporting.ipynb',\n",
       " 'model.h5',\n",
       " 'protos',\n",
       " 'protos.zip']"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from PIL import Image\n",
    "\n",
    "import keras\n",
    "import tensorflow\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D, Conv2D\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.metrics import categorical_crossentropy\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "import os\n",
    "import time\n",
    "from IPython import display\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import class_weight\n",
    "\n",
    "import itertools\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "\n",
    "os.listdir()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the directory structure\n",
    "In these folders we will store the images that will later be fed to the Keras generators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileExistsError",
     "evalue": "[WinError 183] Kan geen bestand maken dat al bestaat: 'base_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileExistsError\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-49-f0d6492771ea>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create a new directory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'base_dir'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileExistsError\u001b[0m: [WinError 183] Kan geen bestand maken dat al bestaat: 'base_dir'"
     ]
    }
   ],
   "source": [
    "# Create a new directory\n",
    "base_dir = 'base_dir'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "\n",
    "#[CREATE FOLDERS INSIDE THE BASE DIRECTORY]\n",
    "\n",
    "# now we create 7 folders inside 'base_dir':\n",
    "\n",
    "# train_dir\n",
    "    # Meaty\n",
    "    # Sweet\n",
    "    # Piquant\n",
    "    # Sour\n",
    "    # Salty\n",
    "    # Bitter\n",
    " \n",
    "# val_dir\n",
    "    # Meaty\n",
    "    # Sweet\n",
    "    # Piquant\n",
    "    # Sour\n",
    "    # Salty\n",
    "    # Bitter\n",
    "\n",
    "# create a path to 'base_dir' to which we will join the names of the new folders\n",
    "# train_dir\n",
    "train_dir = os.path.join(base_dir, 'train_dir')\n",
    "os.mkdir(train_dir)\n",
    "\n",
    "# val_dir\n",
    "val_dir = os.path.join(base_dir, 'val_dir')\n",
    "os.mkdir(val_dir)\n",
    "\n",
    "\n",
    "# [CREATE FOLDERS INSIDE THE TRAIN, VALIDATION AND TEST FOLDERS]\n",
    "# Inside each folder we create seperate folders for each class\n",
    "\n",
    "# create new folders inside train_dir\n",
    "meaty = os.path.join(train_dir, 'Meaty')\n",
    "os.mkdir(meaty)\n",
    "sweet = os.path.join(train_dir, 'Sweet')\n",
    "os.mkdir(sweet)\n",
    "piq = os.path.join(train_dir, 'Piquant')\n",
    "os.mkdir(piq)\n",
    "sour = os.path.join(train_dir, 'Sour')\n",
    "os.mkdir(sour)\n",
    "salty = os.path.join(train_dir, 'Salty')\n",
    "os.mkdir(salty)\n",
    "bitter = os.path.join(train_dir, 'Bitter')\n",
    "os.mkdir(bitter)\n",
    "\n",
    "\n",
    "\n",
    "# create new folders inside val_dir\n",
    "meaty = os.path.join(val_dir, 'Meaty')\n",
    "os.mkdir(meaty)\n",
    "sweet = os.path.join(val_dir, 'Sweet')\n",
    "os.mkdir(sweet)\n",
    "piq = os.path.join(val_dir, 'Piquant')\n",
    "os.mkdir(piq)\n",
    "sour = os.path.join(val_dir, 'Sour')\n",
    "os.mkdir(sour)\n",
    "salty = os.path.join(val_dir, 'Salty')\n",
    "os.mkdir(salty)\n",
    "bitter = os.path.join(val_dir, 'Bitter')\n",
    "os.mkdir(bitter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Train and Val Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read metadata\n",
    "\n",
    "df_meta = pd.read_csv('metadata27368.csv')\n",
    "#df_meta.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Meaty', 'Sweet', 'Piquant', 'Sour', 'Salty', 'Bitter'], dtype='object')\n",
      "[[   nan    nan    nan    nan    nan    nan]\n",
      " [0.8333 0.3333 0.5    0.5    0.8333 0.8333]\n",
      " [0.5    0.3333 0.     0.8333 0.3333 1.    ]\n",
      " ...\n",
      " [0.6667 0.6667 0.     0.     1.     0.6667]\n",
      " [0.5    0.8333 0.5    0.5    0.6667 1.    ]\n",
      " [   nan    nan    nan    nan    nan    nan]]\n",
      "[[   nan    nan    nan    nan    nan    nan]\n",
      " [0.8333 0.8333 0.5    0.8333 0.5    0.3333]\n",
      " [1.     0.5    0.     0.3333 0.8333 0.3333]\n",
      " ...\n",
      " [0.6667 0.6667 0.     1.     0.     0.6667]\n",
      " [1.     0.5    0.5    0.6667 0.5    0.8333]\n",
      " [   nan    nan    nan    nan    nan    nan]]\n",
      "[0 0 0 ... 3 0 0]\n"
     ]
    }
   ],
   "source": [
    "df_flav = pd.read_csv('flavours.csv')\n",
    "print(df_flav.columns)\n",
    "flav = df_flav.values\n",
    "print(flav)\n",
    "# rearrange columns in alphabetical order of flavours (to match those used in data augmentation)\n",
    "your_permutation = [1,5,2,4,3,0]\n",
    "i = np.argsort(your_permutation)\n",
    "flav = flav[:,i]\n",
    "print(flav)\n",
    "encoded_flav = np.argmax(flav, axis=1)\n",
    "print(encoded_flav)\n",
    "df_flav['y'] = encoded_flav"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262 faulty images\n"
     ]
    }
   ],
   "source": [
    "# import faulty images indexes\n",
    "\n",
    "file = open(\"faulty_images.txt\", \"r\")\n",
    "faul_id = []\n",
    "for val in file.read().split():\n",
    "    faul_id.append(int(val)-1)\n",
    "file.close()\n",
    "print(len(faul_id),\"faulty images\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4886 unlabeled images\n"
     ]
    }
   ],
   "source": [
    "# get the unlabeled images indexes\n",
    "\n",
    "# search for nan values in flavours df\n",
    "nan_bool = df_flav.isnull().any(axis=1)\n",
    "nan_id = np.where(nan_bool == True)[0]\n",
    "print(len(nan_id),'unlabeled images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4827 true unlabeled images\n"
     ]
    }
   ],
   "source": [
    "# remove faulty images from unlabeles\n",
    "\n",
    "f_nan_id = []\n",
    "for i in faul_id:\n",
    "    t = np.where(nan_id == i)[0]\n",
    "    if len(t) != 0:\n",
    "        f_nan_id.append(t)\n",
    "nan_id = np.delete(nan_id, f_nan_id)\n",
    "\n",
    "print(len(nan_id),'true unlabeled images')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22549\n"
     ]
    }
   ],
   "source": [
    "# Good images !\n",
    "\n",
    "good_id = np.arange(27638)\n",
    "good_id = np.delete(good_id, np.union1d(faul_id, nan_id))\n",
    "\n",
    "print(len(good_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    5714\n",
      "4    5019\n",
      "3    3516\n",
      "1    3507\n",
      "5    3274\n",
      "2    1519\n",
      "Name: y, dtype: int64\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(df_flav['y'].iloc[good_id].value_counts())\n",
    "\n",
    "y = df_flav['y'].iloc[good_id]\n",
    "\n",
    "plt.hist(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train 18039\n",
      "Val 4510\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Train and validation split \n",
    "train_id, val_id, _, _ = train_test_split(good_id, good_id, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "print('Train', len(train_id))\n",
    "print('Val',len(val_id))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYAAAAEICAYAAABWJCMKAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAAFthJREFUeJzt3X+w3XWd3/Hna4OwCqsgRIsJa9iakR+2/miKbG0tAw6/a9gZ3AEdjJht6gzuYrWjYdsd1l+7ON0urjPqDpWs6AoR0Y5UaNmUH3W2s6DhR9GQdYmAEIkSNwFxqWj03T/O58Lh5iT33Jubc8+93+dj5s79fj/fz/d7Pt87n3Nf5/vjc76pKiRJ3fMrc90ASdLcMAAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDIAFLMmfJ/mDuW6HBJDkpCRb++Y3JTlpmLozeC37/hAOmOsGaM+SPAT8TlX9r5msX1Xvmt0WSbOnqo6fje0keQe998m/7Nu2fX8IHgHMU0kMb0n7xAAYU0k+D/w68N+T/CTJ+5NUktVJHgZuafW+lOQHSZ5I8vUkx/dt47NJPtKmT0qyNcn7kjyWZFuSC+dk5zSvJVmb5LpJZX+W5BNJLkyyOcmTSR5I8u/2sp2HkrypTT+/9dedSe4D/vmA1/xu2+59SX6rlR8L/Dnwm+198ngrf6bvt/l/m2RLkh1Jrk/ysr5lleRdSe5vr//JJJmFP9XYMwDGVFVdADwM/JuqOgS4ti3618CxwGlt/n8Ay4GXAHcBX9jLZv8R8CJgCbAa+GSSw2a/9VrgrgHOTPJCgCSLgN8GrgYeA84GXghcCFye5HVDbPNS4B+3n9OAVZOWfxf4V/T67weBv0xyZFVtBt4F/E1VHVJVh07ecJKTgT9ubTwS+B6wflK1s+mFzqtbvdPoAANg/vnDqvqHqvp/AFW1rqqerKqngT8EXp3kRXtY9+fAh6rq51V1I/AT4JUjabUWjKr6Hr0PG+e0opOBp6rq9qq6oaq+Wz3/G/grev+4p/LbwEerakdVPQJ8YtJrfqmqHq2qX1bVF4H7gROGbPLbgHVVdVd7n1xC74hhWV+dy6rq8ap6GLgVeM2Q257XDID555GJiSSLklzWDo1/DDzUFh2xh3X/vqp29c0/BRyyf5qpBe5q4Pw2/dY2T5IzktzeTrU8DpzJnvtjv5fR17fpfUp/RpK3J7knyeNtu68acrsT235me1X1E+Dv6R0JT/hB33Rn3hcGwHgb9FWt/WVvBVYCb6J3aLyslXfi/KXm1JeAk5IsBX4LuDrJQcCXgT8BXtpOx9zIcP1xG3BU3/yvT0wkeTnwX4F3A4e37X67b7tTfaXxo8DL+7Z3MHA48P0h2rWgGQDj7YfAb+xl+a8BT9P7NPMC4I9G0SipqrYDtwF/ATzYzsUfCBwEbAd2JTkDOHXITV4LXJLksBYqv9u37GB6/+S3A7SbF17Vt/yHwNIkB+5h21cDFyZ5TQupPwLuqKqHhmzbgmUAjLc/Bv5TO+Q9d8Dyz9E7tP0+cB9w+wjbJl1N7+jzaoCqehL4PXr/zHfSO0K9fshtfZBeX36Q3nWDz08sqKr7gP8C/A29f/b/BPg/feveAmwCfpDkR5M3XFU3A39A7+hkG70LzecN2a4FLT4QRpK6ySMASeooA0CdlWRdGxT37b6y/5zkb5Pcm+S/JTm0b9klbTDRd5Kc1ld+eivbkmTtqPdDmikDQF32WeD0SWUbgFdV1T8F/o7ePeMkOY7eeePj2zqfarfhLgI+CZwBHAec3+pKY88AUGdV1deBHZPK/qpvrMTtwNI2vRJYX1VPV9WDwBZ6A5FOALZU1QNV9TN6I0xXjmQHpH001l8odsQRR9SyZcvmuhla2HbQuzNkkHcCX2zTS3juXVZbeXYg0SOTyl8/aGNJ1gBrAA4++OB/dswxx8ywydLe3XnnnT+qqsVT1RvrAFi2bBkbN26c62ZoAUvyfQYcCSf5j8Aunv1upUGDmWrQuuxhYFJVXQFcAbBixYqyb2t/SfK9qWuNeQBIcyHJKnpfDnZKPXuf9FaeO1J1Kb0RpuylXBprXgOQ+iQ5HfgA8Oaqeqpv0fXAeUkOSnI0vW9g/QbwTWB5kqPbSNTzGH7wkzSnPAJQZ51//vkAxwBJ7/GDl9K76+cgYEP7Svjbq+pdVbUpybX0RlzvAi6qql/QW/ndwE3AInrfOrlp5DsjzcBYjwT2PKn2tyR3VtWKUb+ufVv707D92lNAktRRBoAkdZQBIEkdZQBIUkcZAJLUUfP2NtBla2+Y0XoPXXbWLLdEml32bY2KRwCS1FEGgCR1lAEgSR1lAEhSRxkAktRRBoAkdZQBIEkdZQBIUkcZAJLUUQaAJHWUASBJHWUASFJHGQCS1FEGgCR1lAEgSR1lAEhSRxkAktRRBoAkdZQBIEkdZQBIUkcZAJLUUUMHQJJFSe5O8rU2f3SSO5Lcn+SLSQ5s5Qe1+S1t+bK+bVzSyr+T5LTZ3hlpOt75zncCvDrJtyfKkrw4yYbWrzckOayVJ8knWv+9N8nr+tZZ1erfn2TV6PdEmpnpHAFcDGzum/8YcHlVLQd2Aqtb+WpgZ1W9Ari81SPJccB5wPHA6cCnkizat+ZLM/eOd7wD4P5JxWuBm1u/vrnNA5wBLG8/a4BPQy8wgEuB1wMnAJdOhIY07oYKgCRLgbOAz7T5ACcD17UqVwHntOmVbZ62/JRWfyWwvqqerqoHgS303jDSnHjjG98IsGtScX//ndyvP1c9twOHJjkSOA3YUFU7qmonsIHeBxxp7B0wZL2PA+8Hfq3NHw48XlUTb56twJI2vQR4BKCqdiV5otVfAtzet83+ddQhy9beMKP1HrrsrFluyUAvraptAFW1LclLWvkz/bqZ6L97KlcHzaRvj6hfDzTlEUCSs4HHqurO/uIBVWuKZXtbp//11iTZmGTj9u3bp2qeNCr71K/Bvq3xM8wpoDcAb07yELCe3qmfj9M7BJ44glgKPNqmtwJHAbTlLwJ29JcPWOcZVXVFVa2oqhWLFy+e9g5J++iH7dQO7fdjrXxP/Xeofg32bY2fKQOgqi6pqqVVtYzeRdxbquptwK3Aua3aKuCrbfr6Nk9bfktVVSs/r90ldDS9i2nfmLU9kWZHf/+d3K/f3u4GOhF4op0qugk4Nclh7eLvqa1MGnvDXgMY5APA+iQfAe4GrmzlVwKfT7KF3if/8wCqalOSa4H76F14u6iqfrEPry/tk/PPPx/gGHr3NWyldzfPZcC1SVYDDwNvadVvBM6kd/PCU8CFAFW1I8mHgW+2eh+qqh0j2wlpH0wrAKrqNuC2Nv0AA+7iqaqf8uybZvKyjwIfnW4jpf3hmmuuYf369fdW1YpJi06ZXLcdxV40aDtVtQ5Ytx+aKO1XjgSWpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkAZI8u+TbEry7STXJPnVJEcnuSPJ/Um+mOTAVvegNr+lLV82t62XhmMASJMkWQL8HrCiql4FLALOAz4GXF5Vy4GdwOq2ympgZ1W9Ari81ZPGngEgDXYA8PwkBwAvALYBJwPXteVXAee06ZVtnrb8lCQZYVulGTEApEmq6vvAnwAP0/vH/wRwJ/B4Ve1q1bYCS9r0EuCRtu6uVv/wydtNsibJxiQbt2/fvn93QhqCASBNkuQwep/qjwZeBhwMnDGgak2sspdlzxZUXVFVK6pqxeLFi2erudKMGQDS7t4EPFhV26vq58BXgH8BHNpOCQEsBR5t01uBowDa8hcBO0bbZGn6DABpdw8DJyZ5QTuXfwpwH3ArcG6rswr4apu+vs3Tlt9SVbsdAUjjxgCQJqmqO+hdzL0L+Ba998kVwAeA9ybZQu8c/5VtlSuBw1v5e4G1I2+0NAMHTF1F6p6quhS4dFLxA8AJA+r+FHjLKNolzaYpjwDaAJhvJPm/bWDMB1v5tAfFJLmklX8nyWn7a6ckSVMb5hTQ08DJVfVq4DXA6UlOZJqDYpIcR28wzfHA6cCnkiyazZ2RJA1vygConp+02ee1n2L6g2JWAuur6umqehDYwoDDaUnSaAx1ETjJoiT3AI8BG4DvMv1BMc+UD1in/7UcLCNJIzBUAFTVL6rqNfTufT4BOHZQtfZ7T4NiHCwjSWNkWreBVtXjwG3AiUx/UMwz5QPWkSSN2DB3AS1Ocmibfj69UZKbmf6gmOuB89pdQkcDy4FvzNaOSJKmZ5hxAEcCV7U7dn4FuLaqvpbkPmB9ko8Ad/PcQTGfb4NidtC784eq2pTkWnojKncBF1XVL2Z3dyRJw5oyAKrqXuC1A8qnPSimqj4KfHT6zZQkzTa/CkKSOsoAkKSO8ruAxLK1N8xovYcuO2uWWyLNLvv23hkAQ7IjaaGaSd+2Xy8MngKSpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKMcBaN5wLIYWorns1x4BSFJHGQCS1FEGgCR1lAEgSR1lAEhSRxkAktRRBoA0QJJDk1yX5G+TbE7ym0lenGRDkvvb78Na3ST5RJItSe5N8rq5br80DANAGuzPgP9ZVccArwY2A2uBm6tqOXBzmwc4A1jeftYAnx59c6XpMwCkSZK8EHgjcCVAVf2sqh4HVgJXtWpXAee06ZXA56rnduDQJEeOuNnStBkA0u5+A9gO/EWSu5N8JsnBwEurahtA+/2SVn8J8Ejf+ltb2XMkWZNkY5KN27dv3797IA3BAJB2dwDwOuDTVfVa4B949nTPIBlQVrsVVF1RVSuqasXixYtnp6XSPjAApN1tBbZW1R1t/jp6gfDDiVM77fdjffWP6lt/KfDoiNoqzZgBIE1SVT8AHknyylZ0CnAfcD2wqpWtAr7apq8H3t7uBjoReGLiVJE0zvw2UGmw3wW+kORA4AHgQnofmK5Nshp4GHhLq3sjcCawBXiq1ZXGngEgDVBV9wArBiw6ZUDdAi7a742SZpmngCSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjpgyAJEclubU9FWlTkotb+bSfjpRkVat/f5JVe3pNSdL+N8wRwC7gfVV1LHAicFGS45jm05GSvBi4FHg9cAJw6URoSJJGb8oAqKptVXVXm36S3qPxljD9pyOdBmyoqh1VtRPYAJw+q3sjSRratK4BJFkGvBa4g+k/HcmnJknSGBk6AJIcAnwZeE9V/XhvVQeU1V7Kn1vgU5MkaSSGCoAkz6P3z/8LVfWVVjzdpyP51CRJGiPD3AUU4Epgc1X9ad+i6T4d6Sbg1CSHtYu/p7YySdIcGOaBMG8ALgC+leSeVvb7wGVM4+lIVbUjyYeBb7Z6H6qqHbOyF5KkaZsyAKrqrxl8/h6m+XSkqloHrJtOAyVJ+4cjgSWpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABpD5IsSnJ3kq+1+aOT3NGeaPfFJAe28oPa/Ja2fNlctlsalgEg7dnF9B6ANOFjwOXtKXg7gdWtfDWws6peAVze6kljzwCQBkiyFDgL+EybD3AycF2rMvkpeBNPx7sOOKXVl8aaASAN9nHg/cAv2/zhwONVtavN9z/R7pmn3bXlT7T6z+HT7jRuDABpkiRnA49V1Z39xQOq1hDLni3waXcaM8M8D0DqmjcAb05yJvCrwAvpHREcmuSA9im//4l2E0+725rkAOBFgM+60NjzCECapKouqaqlVbUMOA+4pareBtwKnNuqTX4K3sTT8c5t9Xc7ApDGjQEgDe8DwHuTbKF3jv/KVn4lcHgrfy+wdo7aJ02Lp4Ckvaiq24Db2vQDwAkD6vyUZx+JKs0bHgFIUkcZAJLUUQaAJHWUASBJHWUASFJHGQCS1FEGgCR1lAEgSR1lAEhSRxkAktRRBoAkdZQBIEkd5ZfBjalla2+Y9joPXXbWfmiJNHtm0q/Bvr2/eAQgSR1lAEhSRxkAktRRBoAkdZQBIEkdNWUAJFmX5LEk3+4re3GSDUnub78Pa+VJ8okkW5Lcm+R1feusavXvT7Jq0GtJkkZnmCOAzwKnTypbC9xcVcuBm3n2IdhnAMvbzxrg09ALDOBS4PX0nql66URoSJLmxpQBUFVfB3ZMKl4JXNWmrwLO6Sv/XPXcDhya5EjgNGBDVe2oqp3ABnYPFUnSCM30GsBLq2obQPv9kla+BHikr97WVran8t0kWZNkY5KN27dvn2HzJElTme2LwBlQVnsp372w6oqqWlFVKxYvXjyrjZMkPWumAfDDdmqH9vuxVr4VOKqv3lLg0b2US5LmyEwD4Hpg4k6eVcBX+8rf3u4GOhF4op0iugk4Nclh7eLvqa1MkjRHpvwyuCTXACcBRyTZSu9unsuAa5OsBh4G3tKq3wicCWwBngIuBKiqHUk+DHyz1ftQVU2+sCxJGqEpA6Cqzt/DolMG1C3goj1sZx2wblqtkyTtN44EliZJclSSW5NsTrIpycWtfNoDIKVxZgBIu9sFvK+qjgVOBC5KchzTHAApjTsDQJqkqrZV1V1t+klgM71xK9MdACmNNQNA2osky4DXAncw/QGQ0lgzAKQ9SHII8GXgPVX1471VHVC220BHR7lr3BgA0gBJnkfvn/8XquorrXi6AyCfw1HuGjcGgDRJkgBXApur6k/7Fk13AKQ01qYcByB10BuAC4BvJbmnlf0+0xwAKY07A0CapKr+msHn9WGaAyClceYpIEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI6ygCQpI4yACSpowwASeooA0CSOsoAkKSOMgAkqaMMAEnqKANAkjrKAJCkjjIAJKmjRh4ASU5P8p0kW5KsHfXrS/uD/Vrz0UgDIMki4JPAGcBxwPlJjhtlG6TZZr/WfDXqI4ATgC1V9UBV/QxYD6wccRuk2Wa/1ryUqhrdiyXnAqdX1e+0+QuA11fVu/vqrAHWtNlXAt/Zw+aOAH60H5s71xby/o3Tvr28qhbvywaG6det3L69sPcNxmf/hurXB4yiJX0yoOw5CVRVVwBXTLmhZGNVrZitho2bhbx/C3DfpuzXYN+Ghb1vMP/2b9SngLYCR/XNLwUeHXEbpNlmv9a8NOoA+CawPMnRSQ4EzgOuH3EbpNlmv9a8NNJTQFW1K8m7gZuARcC6qto0w81NeSg9zy3k/VtQ+zbL/RoW2N9nkoW8bzDP9m+kF4ElSePDkcCS1FEGgCR11LwMgIU67D7JUUluTbI5yaYkF891m2ZbkkVJ7k7ytbluy7hZqP0a7Nvjat4FwAIfdr8LeF9VHQucCFy0gPZtwsXA5rluxLhZ4P0a7Ntjad4FAAt42H1Vbauqu9r0k/Q605K5bdXsSbIUOAv4zFy3ZQwt2H4N9u1xNR8DYAnwSN/8VhZQR5qQZBnwWuCOuW3JrPo48H7gl3PdkDHUiX4N9u1xMh8DYKhh9/NZkkOALwPvqaofz3V7ZkOSs4HHqurOuW7LmFrw/Rrs2+NmPgbAgh52n+R59N4gX6iqr8x1e2bRG4A3J3mI3umNk5P85dw2aaws6H4N9u1xNO8GgiU5APg74BTg+/SG4b91H0dejoUkAa4CdlTVe+a6PftLkpOA/1BVZ891W8bFQu7XYN8eV/PuCKCqdgETw+43A9culDcJvU8SF9D7BHFP+zlzrhul/W+B92uwb4+leXcEIEmaHfPuCECSNDsMAEnqKANAkjrKAJCkjjIAJKmjDABJ6igDQJI66v8DYr6y1+kXGnoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# train and val distibution\n",
    "\n",
    "plt.subplot(121)\n",
    "plt.hist(df_flav['y'].iloc[train_id])\n",
    "plt.title('train')\n",
    "plt.subplot(122)\n",
    "plt.hist(df_flav['y'].iloc[val_id])\n",
    "plt.title('validation')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transfer the Images into the Folders\n",
    "\n",
    "### Bitter --> 0, Meaty --> 1, Piquant --> 2, Salty --> 3, Sour --> 4, Sweet --> 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def y2label(y):\n",
    "    if y == 0:\n",
    "        label = 'Bitter'\n",
    "    elif y == 1:\n",
    "        label = 'Meaty'\n",
    "    elif y == 2:\n",
    "        label = 'Piquant'\n",
    "    elif y == 3:\n",
    "        label = 'Salty'\n",
    "    elif y == 4:\n",
    "        label = 'Sour'\n",
    "    else:\n",
    "        label = 'Sweet'\n",
    "    return label\n",
    "\n",
    "def label2y(label):\n",
    "    if label == 'Bitter':\n",
    "        y = 0\n",
    "    elif label == 'Meaty':\n",
    "        y = 1\n",
    "    elif label == 'Piquant':\n",
    "        y = 2\n",
    "    elif label == 'Salty':\n",
    "        y = 3\n",
    "    elif label == 'Sour':\n",
    "        y = 4\n",
    "    else:\n",
    "        y = 5\n",
    "    return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-60-491f0a24f3cc>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m     \u001b[0msrc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg_folder\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;31m# destination path to image\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m     \u001b[0mdst\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m     \u001b[1;31m# copy the image from the source to the destination\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mshutil\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcopyfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdst\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_dir' is not defined"
     ]
    }
   ],
   "source": [
    "# image folder\n",
    "img_folder = \"images27638/\"\n",
    "\n",
    "# images indexes (name): train_id, val_id\n",
    "\n",
    "\n",
    "# Transfer the train images\n",
    "for i in train_id:\n",
    "    im = str(i+1)\n",
    "    while len(im) < 5 :\n",
    "        im = '0' + im\n",
    "    im ='img' + im + '.jpg'\n",
    "    fname = im\n",
    "    \n",
    "    y_i = df_flav['y'].iloc[i]\n",
    "    label = y2label(y_i)\n",
    "    \n",
    "    # source path to image\n",
    "    src = os.path.join(img_folder, fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(train_dir, label, fname)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Transfer the val images\n",
    "for i in val_id:\n",
    "    im = str(i+1)\n",
    "    while len(im) < 5 :\n",
    "        im = '0' + im\n",
    "    im ='img' + im + '.jpg'\n",
    "    fname = im\n",
    "    \n",
    "    y_i = df_flav['y'].iloc[i]\n",
    "    label = y2label(y_i)\n",
    "    \n",
    "    # source path to image\n",
    "    src = os.path.join(img_folder, fname)\n",
    "    # destination path to image\n",
    "    dst = os.path.join(val_dir, label, fname)\n",
    "    # copy the image from the source to the destination\n",
    "    shutil.copyfile(src, dst)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4606\n",
      "4619\n",
      "4645\n",
      "4615\n",
      "4613\n",
      "4571\n"
     ]
    }
   ],
   "source": [
    "# check how many train images we have in each folder\n",
    "\n",
    "print(len(os.listdir('base_dir/train_dir/Meaty/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Sweet/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Piquant/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Sour/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Salty/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Bitter/')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "701\n",
      "655\n",
      "304\n",
      "1004\n",
      "703\n",
      "1143\n"
     ]
    }
   ],
   "source": [
    "# check how many val images we have in each folder\n",
    "\n",
    "print(len(os.listdir('base_dir/val_dir/Meaty/')))\n",
    "print(len(os.listdir('base_dir/val_dir/Sweet/')))\n",
    "print(len(os.listdir('base_dir/val_dir/Piquant/')))\n",
    "print(len(os.listdir('base_dir/val_dir/Sour/')))\n",
    "print(len(os.listdir('base_dir/val_dir/Salty/')))\n",
    "print(len(os.listdir('base_dir/val_dir/Bitter/')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Copy the train images  into aug_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4606 images belonging to 1 classes.\n",
      "Found 4619 images belonging to 1 classes.\n",
      "Found 4645 images belonging to 1 classes.\n",
      "Found 4613 images belonging to 1 classes.\n",
      "Found 4615 images belonging to 1 classes.\n"
     ]
    }
   ],
   "source": [
    "# note that we are not augmenting class 'Bitter'\n",
    "class_list = ['Meaty','Sweet','Piquant','Salty','Sour']\n",
    "\n",
    "for item in class_list:\n",
    "    \n",
    "    # We are creating temporary directories here because we delete these directories later\n",
    "    # create a base dir\n",
    "    aug_dir = 'aug_dir'\n",
    "    os.mkdir(aug_dir)\n",
    "    # create a dir within the base dir to store images of the same class\n",
    "    img_dir = os.path.join(aug_dir, 'img_dir')\n",
    "    os.mkdir(img_dir)\n",
    "\n",
    "    # Choose a class\n",
    "    img_class = item\n",
    "\n",
    "    # list all images in that directory\n",
    "    img_list = os.listdir('base_dir/train_dir/' + img_class)\n",
    "\n",
    "    # Copy images from the class train dir to the img_dir e.g. class 'mel'\n",
    "    for fname in img_list:\n",
    "            # source path to image\n",
    "            src = os.path.join('base_dir/train_dir/' + img_class, fname)\n",
    "            # destination path to image\n",
    "            dst = os.path.join(img_dir, fname)\n",
    "            # copy the image from the source to the destination\n",
    "            shutil.copyfile(src, dst)\n",
    "\n",
    "\n",
    "    # point to a dir containing the images and not to the images themselves\n",
    "    path = aug_dir\n",
    "    save_path = 'base_dir/train_dir/' + img_class\n",
    "\n",
    "    # Create a data generator\n",
    "    datagen = ImageDataGenerator(\n",
    "        rotation_range=90,\n",
    "        width_shift_range=0.1,\n",
    "        height_shift_range=0.1,\n",
    "        zoom_range=0.1,\n",
    "        horizontal_flip=True,\n",
    "        vertical_flip=True,\n",
    "        brightness_range=(0.9,1.1),\n",
    "        fill_mode='nearest')\n",
    "\n",
    "    batch_size = 50\n",
    "\n",
    "    aug_datagen = datagen.flow_from_directory(path,\n",
    "                                           save_to_dir=save_path,\n",
    "                                           save_format='jpg',\n",
    "                                                    target_size=(224,224),\n",
    "                                                    batch_size=batch_size)\n",
    "\n",
    "\n",
    "\n",
    "    # Generate the augmented images and add them to the training folders\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    num_aug_images_wanted = 4600 # total number of images we want to have in each class\n",
    "    \n",
    "    ###########\n",
    "    \n",
    "    num_files = len(os.listdir(img_dir))\n",
    "    num_batches = int(np.ceil((num_aug_images_wanted-num_files)/batch_size))\n",
    "\n",
    "    # run the generator and create about 6000 augmented images\n",
    "    for i in range(0,num_batches):\n",
    "\n",
    "        imgs, labels = next(aug_datagen)\n",
    "        \n",
    "    # delete temporary directory with the raw image files\n",
    "    shutil.rmtree('aug_dir')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4606\n",
      "4619\n",
      "4645\n",
      "4615\n",
      "4613\n",
      "4571\n"
     ]
    }
   ],
   "source": [
    "# Check how many train images we now have in each folder.\n",
    "# This is the original images plus the augmented images.\n",
    "\n",
    "print(len(os.listdir('base_dir/train_dir/Meaty/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Sweet/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Piquant/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Sour/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Salty/')))\n",
    "print(len(os.listdir('base_dir/train_dir/Bitter/')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Set Up the Generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_path = 'base_dir/train_dir'\n",
    "valid_path = 'base_dir/val_dir'\n",
    "\n",
    "num_train_samples = len(train_id)\n",
    "num_val_samples = len(val_id)\n",
    "train_batch_size = 128\n",
    "val_batch_size = 128\n",
    "image_size = 224\n",
    "\n",
    "train_steps = np.ceil(num_train_samples / train_batch_size)\n",
    "val_steps = np.ceil(num_val_samples / val_batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 27669 images belonging to 6 classes.\n",
      "Found 4510 images belonging to 6 classes.\n",
      "Found 4510 images belonging to 6 classes.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function= \\\n",
    "    tensorflow.keras.applications.mobilenet.preprocess_input)\n",
    "train_batches = datagen.flow_from_directory(train_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=train_batch_size)\n",
    "\n",
    "valid_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=val_batch_size)\n",
    "\n",
    "# Note: shuffle=False causes the test dataset to not be shuffled\n",
    "test_batches = datagen.flow_from_directory(valid_path,\n",
    "                                            target_size=(image_size,image_size),\n",
    "                                            batch_size=1,\n",
    "                                            shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modify MobileNet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Attempted to use a closed Session.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-74e93d1d3a22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# create a copy of a mobilenet model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmobile\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMobileNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput_shape\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m224\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;31m#mobile = tensorflow.keras.applications.DenseNet169(weights='imagenet',input_shape=(224,224,3))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m#mobile = tensorflow.keras.applications.MobileNetV2(weights='imagenet',input_shape=(224,224,3))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;31m#dense_net = tensorflow.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=(224,224,3))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\__init__.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     68\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'models'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodels\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m       \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'utils'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 70\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mbase_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     71\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     72\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\applications\\mobilenet.py\u001b[0m in \u001b[0;36mMobileNet\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m@\u001b[0m\u001b[0mkeras_modules_injection\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     31\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mMobileNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 32\u001b[1;33m   \u001b[1;32mreturn\u001b[0m \u001b[0mmobilenet\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mMobileNet\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     34\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\keras_applications\\mobilenet.py\u001b[0m in \u001b[0;36mMobileNet\u001b[1;34m(input_shape, alpha, depth_multiplier, dropout, include_top, weights, input_tensor, pooling, classes, **kwargs)\u001b[0m\n\u001b[0;32m    318\u001b[0m                                                 \u001b[0mweight_path\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                                                 cache_subdir='models')\n\u001b[1;32m--> 320\u001b[1;33m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    321\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mweights\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    322\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\network.py\u001b[0m in \u001b[0;36mload_weights\u001b[1;34m(self, filepath, by_name)\u001b[0m\n\u001b[0;32m   1514\u001b[0m         \u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights_from_hdf5_group_by_name\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1515\u001b[0m       \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1516\u001b[1;33m         \u001b[0msaving\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_weights_from_hdf5_group\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1517\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1518\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_updated_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\engine\\saving.py\u001b[0m in \u001b[0;36mload_weights_from_hdf5_group\u001b[1;34m(f, layers)\u001b[0m\n\u001b[0;32m    818\u001b[0m                        str(len(weight_values)) + ' elements.')\n\u001b[0;32m    819\u001b[0m     \u001b[0mweight_value_tuples\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msymbolic_weights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight_values\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 820\u001b[1;33m   \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight_value_tuples\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    821\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    822\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[1;34m(tuples)\u001b[0m\n\u001b[0;32m   2878\u001b[0m           \u001b[0massign_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2879\u001b[0m           \u001b[0mfeed_dict\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0massign_placeholder\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2880\u001b[1;33m         \u001b[0mget_session\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0massign_ops\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2881\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2882\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36mget_session\u001b[1;34m()\u001b[0m\n\u001b[0;32m    480\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0m_MANUAL_VAR_INIT\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    481\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 482\u001b[1;33m       \u001b[0m_initialize_variables\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msession\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    483\u001b[0m   \u001b[1;32mreturn\u001b[0m \u001b[0msession\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    484\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m_initialize_variables\u001b[1;34m(session)\u001b[0m\n\u001b[0;32m    756\u001b[0m     \u001b[1;31m# marked as initialized.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    757\u001b[0m     is_initialized = session.run(\n\u001b[1;32m--> 758\u001b[1;33m         [variables_module.is_variable_initialized(v) for v in candidate_vars])\n\u001b[0m\u001b[0;32m    759\u001b[0m     \u001b[0muninitialized_vars\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    760\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mflag\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_initialized\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcandidate_vars\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m    927\u001b[0m     \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    928\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[1;32m--> 929\u001b[1;33m                          run_metadata_ptr)\n\u001b[0m\u001b[0;32m    930\u001b[0m       \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    931\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[0;32m   1073\u001b[0m     \u001b[1;31m# Check session.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_closed\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1075\u001b[1;33m       \u001b[1;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Attempted to use a closed Session.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1076\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mversion\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1077\u001b[0m       raise RuntimeError('The Session graph is empty.  Add operations to the '\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Attempted to use a closed Session."
     ]
    }
   ],
   "source": [
    "# create a copy of a mobilenet model\n",
    "mobile = tensorflow.keras.applications.MobileNet(input_shape=(224,224,3))\n",
    "#mobile = tensorflow.keras.applications.DenseNet169(weights='imagenet',input_shape=(224,224,3))\n",
    "#mobile = tensorflow.keras.applications.MobileNetV2(weights='imagenet',input_shape=(224,224,3))\n",
    "#dense_net = tensorflow.keras.applications.DenseNet201(weights='imagenet', include_top=False, input_shape=(224,224,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE MODEL ARCHITECTURE\n",
    "\n",
    "# Exclude the last 5 layers of the above model.\n",
    "# This will include all layers up to and including global_average_pooling2d_1\n",
    "x = mobile.layers[-6].output\n",
    "\n",
    "# Create a new dense layer for predictions\n",
    "# 7 corresponds to the number of classes\n",
    "x = Dropout(0.25)(x)\n",
    "predictions = Dense(6, activation='softmax')(x)\n",
    "\n",
    "# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n",
    "# dense layer we created above.\n",
    "\n",
    "model = Model(inputs=mobile.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We need to choose how many layers we actually want to be trained.\n",
    "\n",
    "# Here we are freezing the weights of all layers except the\n",
    "# last 23 layers in the new model.\n",
    "# The last 23 layers of the model will be trained.\n",
    "\n",
    "for layer in model.layers[:-23]:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(Adam(lr=0.01), loss='categorical_crossentropy', \n",
    "              metrics=['acc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "filepath = \"model.h5\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n",
    "                             save_best_only=True, mode='max')\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_acc', factor=0.5, patience=2, \n",
    "                                   verbose=1, mode='max', min_lr=0.00001)\n",
    "                              \n",
    "    \n",
    "callbacks_list = [checkpoint, reduce_lr]\n",
    "\n",
    "history = model.fit_generator(train_batches, steps_per_epoch=train_steps, \n",
    "                    validation_data=valid_batches,\n",
    "                    validation_steps=val_steps,\n",
    "                    epochs=5, verbose=1,\n",
    "                   callbacks=callbacks_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Best model\n",
    "model.load_weights(filepath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the Training Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best validation accuracy:  0.23813747\n",
      "On epoch:  1\n"
     ]
    }
   ],
   "source": [
    "# display the loss and accuracy curves\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "acc = history.history['acc']\n",
    "val_acc = history.history['val_acc']\n",
    "loss = history.history['loss']\n",
    "val_loss = history.history['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "plt.plot(epochs, loss, 'bo', label='Training loss')\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, acc, 'bo', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.legend()\n",
    "plt.figure()\n",
    "plt.show()\n",
    "\n",
    "print('Best validation accuracy: ', np.max(val_acc))\n",
    "print('On epoch: ', epochs[np.where(val_acc == np.max(val_acc))[0][0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Confusion Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# # Get the labels of the test images.\n",
    "\n",
    "# test_labels = test_batches.classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # We need these to plot the confusion matrix.\n",
    "# test_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Print the label associated with each class\n",
    "# test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # make a prediction\n",
    "# predictions = model.predict_generator(test_batches, steps=len(val_id), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predictions.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Source: Scikit Learn website\n",
    "# # http://scikit-learn.org/stable/auto_examples/\n",
    "# # model_selection/plot_confusion_matrix.html#sphx-glr-auto-examples-model-\n",
    "# # selection-plot-confusion-matrix-py\n",
    "\n",
    "\n",
    "# def plot_confusion_matrix(cm, classes,\n",
    "#                           normalize=False,\n",
    "#                           title='Confusion matrix',\n",
    "#                           cmap=plt.cm.Blues):\n",
    "#     \"\"\"\n",
    "#     This function prints and plots the confusion matrix.\n",
    "#     Normalization can be applied by setting `normalize=True`.\n",
    "#     \"\"\"\n",
    "#     if normalize:\n",
    "#         cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "#         print(\"Normalized confusion matrix\")\n",
    "#     else:\n",
    "#         print('Confusion matrix, without normalization')\n",
    "\n",
    "#     print(cm)\n",
    "\n",
    "#     plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "#     plt.title(title)\n",
    "#     plt.colorbar()\n",
    "#     tick_marks = np.arange(len(classes))\n",
    "#     plt.xticks(tick_marks, classes, rotation=45)\n",
    "#     plt.yticks(tick_marks, classes)\n",
    "\n",
    "#     fmt = '.2f' if normalize else 'd'\n",
    "#     thresh = cm.max() / 2.\n",
    "#     for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "#         plt.text(j, i, format(cm[i, j], fmt),\n",
    "#                  horizontalalignment=\"center\",\n",
    "#                  color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "#     plt.ylabel('True label')\n",
    "#     plt.xlabel('Predicted label')\n",
    "#     plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # argmax returns the index of the max value in a row\n",
    "# cm = confusion_matrix(test_labels, predictions.argmax(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_batches.class_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Define the labels of the class indices. These need to match the \n",
    "# # order shown above.\n",
    "# cm_plot_labels = ['Bitter', 'Meaty', 'Piquant', 'Salty', 'Sour','Sweet']\n",
    "\n",
    "# plot_confusion_matrix(cm, cm_plot_labels, title='Confusion Matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete temporary directory with the raw image files\n",
    "# shutil.rmtree('base_dir')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test on images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # SAVE ENCODED LAYERS FOR LATER VISUALIZATIONS\n",
    "# # Input the model and a layer to encode, which can be visualized later\n",
    "# def encoder(model,layer):\n",
    "#     selection = 'conv_pw_' + str(layer)\n",
    "#     encoder = Model(inputs=model.input,outputs=model.get_layer(selection).output)\n",
    "#     #encoder.summary()\n",
    "#     return(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_pie_chart(model, img, labels):\n",
    "#     #fig, ax = plt.subplots(figsize=(6, 3), subplot_kw=dict(aspect=\"equal\"))\n",
    "\n",
    "#     recipe = labels\n",
    "\n",
    "#     data = model.predict(img)[0]\n",
    "\n",
    "#     wedges, texts = ax.pie(data, wedgeprops=dict(width=0.5), startangle=-40)\n",
    "\n",
    "#     bbox_props = dict(boxstyle=\"square,pad=0.3\", fc=\"w\", ec=\"k\", lw=0.72)\n",
    "#     kw = dict(xycoords='data', textcoords='data', arrowprops=dict(arrowstyle=\"-\"),\n",
    "#               bbox=bbox_props, zorder=0, va=\"center\")\n",
    "\n",
    "#     for i, p in enumerate(wedges):\n",
    "#         ang = (p.theta2 - p.theta1)/2. + p.theta1\n",
    "#         y = np.sin(np.deg2rad(ang))\n",
    "#         x = np.cos(np.deg2rad(ang))\n",
    "#         horizontalalignment = {-1: \"right\", 1: \"left\"}[int(np.sign(x))]\n",
    "#         connectionstyle = \"angle,angleA=0,angleB={}\".format(ang)\n",
    "#         kw[\"arrowprops\"].update({\"connectionstyle\": connectionstyle})\n",
    "#         ax.annotate(recipe[i], xy=(x, y), xytext=(1.35*np.sign(x), 1.4*y),\n",
    "#                      horizontalalignment=horizontalalignment, **kw)\n",
    "\n",
    "#     ax.set_title(\"What AI sees\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # creat a useful dictionary structures\n",
    "\n",
    "# partition = {}\n",
    "# partition['train'] = train_id\n",
    "# partition['validation'] = val_id\n",
    "# partition['faulty'] = faul_id\n",
    "# partition['no_label'] = nan_id\n",
    "\n",
    "# labels = {}\n",
    "# name = {}\n",
    "# image = {}\n",
    "# for i in range(27638):\n",
    "    \n",
    "#     labels[str(i)] = df_flav.iloc[[i]].values[0] # every label is a panda df rows\n",
    "    \n",
    "#     name[str(i)] = df_meta['name'][i]\n",
    "    \n",
    "#     im = str(i+1)\n",
    "#     while len(im) < 5 :\n",
    "#         im = '0' + im\n",
    "#     im ='img' + im + '.jpg'\n",
    "#     image[str(i)] = im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# num_images = 50\n",
    "# np.random.seed(42)\n",
    "# random_images = random.sample(list(nan_id),num_images)\n",
    "\n",
    "# encoded = encoder(model, 12)\n",
    "\n",
    "\n",
    "\n",
    "# for i, img_id in enumerate(random_images):\n",
    "    \n",
    "#     plt.figure(figsize=(12,12))\n",
    "#     #dsaodskaod\n",
    "    \n",
    "#     #Load in example image\n",
    "#     im = Image.open(\"../input/images27638/images27638/\"+image[str(img_id)])\n",
    "#     im = im.resize((224, 224), Image.ANTIALIAS)\n",
    "#     im.load()\n",
    "#     im = np.asarray(im, dtype=np.uint8)/255\n",
    "\n",
    "#     #Plot original image\n",
    "#     ax = plt.subplot(1, 3, 1)\n",
    "#     plt.imshow(im)\n",
    "#     plt.title('Original')\n",
    "#     im = np.expand_dims(im, axis=0)\n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     ax.get_yaxis().set_visible(False)\n",
    "    \n",
    "#     #[LORENZO] HERE WE WANT TO VISUALIZE THE ENCODED LAYERS\n",
    "#     #Plot encoded image (\"what the AE sees\")\n",
    "#     ax = plt.subplot(1, 3, 2)\n",
    "#     encoded_img = encoded.predict(im)\n",
    "#     #This is still from the autoencoder so we have to reshape differently\n",
    "#     #To plot the image we need to know the dimension of that layer\n",
    "#     #This can be found by multiplication e.g. \n",
    "#     #conv_pw_12 (Conv2D)  (None, 7, 7, 1024)    = 7*7*1024 = 50,176 dimensions\n",
    "#     #We need to reshape the image to fit this dimension e.g. 256 x 196 = 50,176 \n",
    "#     plt.imshow(encoded_img.reshape(256, 196).T)\n",
    "#     plt.title('Encoded')\n",
    "#     ax.get_xaxis().set_visible(False)\n",
    "#     ax.get_yaxis().set_visible(False)\n",
    "\n",
    "#     #Plot decoded image (\"what the AE reconstructed\")\n",
    "#     ax = plt.subplot(1, 3, 3)\n",
    "#     plot_pie_chart(model, im, df_flav.columns)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize layers other approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x):\n",
    "    \"\"\"utility function to normalize a tensor.\n",
    "    # Arguments\n",
    "        x: An input tensor.\n",
    "    # Returns\n",
    "        The normalized input tensor.\n",
    "    \"\"\"\n",
    "    return x / (K.sqrt(K.mean(K.square(x))) + K.epsilon())\n",
    "\n",
    "\n",
    "def deprocess_image(x):\n",
    "    \"\"\"utility function to convert a float array into a valid uint8 image.\n",
    "    # Arguments\n",
    "        x: A numpy-array representing the generated image.\n",
    "    # Returns\n",
    "        A processed numpy-array, which could be used in e.g. imshow.\n",
    "    \"\"\"\n",
    "    # normalize tensor: center on 0., ensure std is 0.25\n",
    "    x -= x.mean()\n",
    "    x /= (x.std() + K.epsilon())\n",
    "    x *= 0.25\n",
    "\n",
    "    # clip to [0, 1]\n",
    "    x += 0.5\n",
    "    x = np.clip(x, 0, 1)\n",
    "\n",
    "    # convert to RGB array\n",
    "    x *= 255\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((1, 2, 0))\n",
    "    x = np.clip(x, 0, 255).astype('uint8')\n",
    "    return x\n",
    "\n",
    "\n",
    "def process_image(x, former):\n",
    "    \"\"\"utility function to convert a valid uint8 image back into a float array.\n",
    "       Reverses `deprocess_image`.\n",
    "    # Arguments\n",
    "        x: A numpy-array, which could be used in e.g. imshow.\n",
    "        former: The former numpy-array.\n",
    "                Need to determine the former mean and variance.\n",
    "    # Returns\n",
    "        A processed numpy-array representing the generated image.\n",
    "    \"\"\"\n",
    "    if K.image_data_format() == 'channels_first':\n",
    "        x = x.transpose((2, 0, 1))\n",
    "    return (x / 255 - 0.5) * 4 * former.std() + former.mean()\n",
    "\n",
    "\n",
    "def visualize_layer(model,\n",
    "                    input_img_arr,\n",
    "                    layer_id,\n",
    "                    filter_id,\n",
    "                    step=1.,\n",
    "                    epochs=15,\n",
    "                    upscaling_steps=9,\n",
    "                    upscaling_factor=1.2,\n",
    "                    output_dim=(224, 224)):\n",
    "    \"\"\"Visualizes the most relevant filters of one conv-layer in a certain model.\n",
    "    # Arguments\n",
    "        model: The model containing layer_name.\n",
    "        layer_name: The name of the layer to be visualized.\n",
    "                    Has to be a part of model.\n",
    "        step: step size for gradient ascent.\n",
    "        epochs: Number of iterations for gradient ascent.\n",
    "        upscaling_steps: Number of upscaling steps.\n",
    "                         Starting image is in this case (80, 80).\n",
    "        upscaling_factor: Factor to which to slowly upgrade\n",
    "                          the image towards output_dim.\n",
    "        output_dim: [img_width, img_height] The output image dimensions.\n",
    "        filter_range: Tupel[lower, upper]\n",
    "                      Determines the to be computed filter numbers.\n",
    "                      If the second value is `None`,\n",
    "                      the last filter will be inferred as the upper boundary.\n",
    "    \"\"\"\n",
    "\n",
    "    def _generate_filter_image(input_img,\n",
    "                              input_im_array,\n",
    "                              layer_output,\n",
    "                              filter_index):\n",
    "        \"\"\"Generates image for one particular filter.\n",
    "        # Arguments\n",
    "            input_img: The input-image Tensor.\n",
    "            layer_output: The output-image Tensor.\n",
    "            filter_index: The to be processed filter number.\n",
    "                          Assumed to be valid.\n",
    "        #Returns\n",
    "            Either None if no image could be generated.\n",
    "            or a tuple of the image (array) itself and the last loss.\n",
    "        \"\"\"\n",
    "        s_time = time.time()\n",
    "\n",
    "        # we build a loss function that maximizes the activation\n",
    "        # of the nth filter of the layer considered\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            loss = K.mean(layer_output[:, filter_index, :, :])\n",
    "        else:\n",
    "            loss = K.mean(layer_output[:, :, :, filter_index])\n",
    "\n",
    "        # we compute the gradient of the input picture wrt this loss\n",
    "        grads = K.gradients(loss, input_img)[0]\n",
    "\n",
    "        # normalization trick: we normalize the gradient\n",
    "        grads = normalize(grads)\n",
    "\n",
    "        # this function returns the loss and grads given the input picture\n",
    "        iterate = K.function([input_img], [loss, grads])\n",
    "\n",
    "        # we start from a gray image with some random noise\n",
    "        intermediate_dim = tuple(\n",
    "            int(x / (upscaling_factor ** upscaling_steps)) for x in output_dim)\n",
    "        if K.image_data_format() == 'channels_first':\n",
    "            input_img_data = np.expand_dims(np.moveaxis(input_im_array, 2, 0), axis=0)\n",
    "        else:\n",
    "            input_img_data = np.expand_dims(input_im_array, axis=0)\n",
    "            \n",
    "        # Slowly upscaling towards the original size prevents\n",
    "        # a dominating high-frequency of the to visualized structure\n",
    "        # as it would occur if we directly compute the 412d-image.\n",
    "        # Behaves as a better starting point for each following dimension\n",
    "        # and therefore avoids poor local minima\n",
    "        for up in reversed(range(upscaling_steps)):\n",
    "            # we run gradient ascent for e.g. 20 steps\n",
    "            for _ in range(epochs):\n",
    "                loss_value, grads_value = iterate([input_img_data])\n",
    "                input_img_data = input_img_data + (grads_value * step)\n",
    "\n",
    "                # some filters get stuck to 0, we can skip them\n",
    "                if loss_value <= K.epsilon():\n",
    "                    return None\n",
    "\n",
    "            # Calulate upscaled dimension\n",
    "            intermediate_dim = tuple(\n",
    "                int(x / (upscaling_factor ** up)) for x in output_dim)\n",
    "            # Upscale\n",
    "            img = deprocess_image(input_img_data[0])\n",
    "            img = np.array(pil_image.fromarray(img).resize(intermediate_dim,\n",
    "                                                           pil_image.BICUBIC))\n",
    "            input_img_data = [process_image(img, input_img_data[0])]\n",
    "\n",
    "        # decode the resulting input image\n",
    "        img = deprocess_image(input_img_data[0])\n",
    "        e_time = time.time()\n",
    "        #print('Costs of filter {:3}: {:5.0f} ( {:4.2f}s )'.format(filter_index,\n",
    "                                                                  #loss_value,\n",
    "                                                                  #e_time - s_time))\n",
    "        return img, loss_value\n",
    "\n",
    "    def _draw_filters(filters, n=None):\n",
    "        \"\"\"Draw the best filters in a nxn grid.\n",
    "        # Arguments\n",
    "            filters: A List of generated images and their corresponding losses\n",
    "                     for each processed filter.\n",
    "            n: dimension of the grid.\n",
    "               If none, the largest possible square will be used\n",
    "        \"\"\"\n",
    "        if n is None:\n",
    "            n = int(np.floor(np.sqrt(len(filters))))\n",
    "\n",
    "        # the filters that have the highest loss are assumed to be better-looking.\n",
    "        # we will only keep the top n*n filters.\n",
    "        filters.sort(key=lambda x: x[1], reverse=True)\n",
    "        filters = filters[:n * n]\n",
    "\n",
    "        # build a black picture with enough space for\n",
    "        # e.g. our 8 x 8 filters of size 412 x 412, with a 5px margin in between\n",
    "        MARGIN = 5\n",
    "        width = n * output_dim[0] + (n - 1) * MARGIN\n",
    "        height = n * output_dim[1] + (n - 1) * MARGIN\n",
    "        stitched_filters = np.zeros((width, height, 3), dtype='uint8')\n",
    "\n",
    "        # fill the picture with our saved filters\n",
    "        for i in range(n):\n",
    "            for j in range(n):\n",
    "                img, _ = filters[i * n + j]\n",
    "                width_margin = (output_dim[0] + MARGIN) * i\n",
    "                height_margin = (output_dim[1] + MARGIN) * j\n",
    "                stitched_filters[\n",
    "                    width_margin: width_margin + output_dim[0],\n",
    "                    height_margin: height_margin + output_dim[1], :] = img\n",
    "\n",
    "        # save the result to disk\n",
    "        #save_img('vgg_{0:}_{1:}x{1:}.png'.format(layer_name, n), stitched_filters)\n",
    "        \n",
    "        plt.imshow(stitched_filters)\n",
    "        plt.show()\n",
    "\n",
    "    # this is the placeholder for the input images\n",
    "    assert len(model.inputs) == 1\n",
    "    input_img = model.inputs[0]\n",
    "\n",
    "    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n",
    "    layer_dict = dict([(layer.name, layer) for layer in model.layers[1:]])\n",
    "    \n",
    "    # get the name of the desired layer\n",
    "    for i,layer in enumerate(model.layers):\n",
    "        if i == layer_id:\n",
    "            output_layer = layer_dict[layer.name]\n",
    "\n",
    "    \n",
    "    assert isinstance(output_layer, layers.Dense)\n",
    "\n",
    "    processed_filters = []\n",
    "    \n",
    "    img_loss = _generate_filter_image(input_img, input_img_arr, output_layer.output, filter_id)\n",
    "\n",
    "    if img_loss is not None:\n",
    "        processed_filters.append(img_loss)\n",
    "\n",
    "    print('{} filter processed.'.format(len(processed_filters)))\n",
    "    # Finally draw and store the best filters to disk\n",
    "    _draw_filters(processed_filters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.models import load_model\n",
    "# model.save('mobile_yummly.h5')\n",
    "# #del model\n",
    "# model_saved = load_model('mobile_yummly.h5')\n",
    "\n",
    "# model_saved.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# im = Image.open(\"../input/images27638/images27638/img01993.jpg\")\n",
    "# im = im.resize((224, 224), Image.ANTIALIAS)\n",
    "# im.load()\n",
    "            \n",
    "# X = np.asarray(im)\n",
    "# X = X.reshape(1,224,224,3)\n",
    "\n",
    "#plt.imshow(X)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize_layer(model_saved,X,1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install keract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keract import get_activations\n",
    "# get_activations(model, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to Protobuf file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\users\\user\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\tensorflow\\python\\saved_model\\signature_def_utils_impl.py:205: build_tensor_info (from tensorflow.python.saved_model.utils_impl) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "This function will only be available through the v1 compatibility library as tf.compat.v1.saved_model.utils.build_tensor_info or tf.compat.v1.saved_model.build_tensor_info.\n",
      "INFO:tensorflow:No assets to save.\n",
      "INFO:tensorflow:No assets to write.\n",
      "INFO:tensorflow:SavedModel written to: protos\\saved_model.pb\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "b'protos\\\\saved_model.pb'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras import backend as K\n",
    "from keras.models import load_model\n",
    "from tensorflow.python.saved_model import builder as saved_model_builder\n",
    "from tensorflow.python.saved_model.signature_def_utils import predict_signature_def\n",
    "from tensorflow.python.saved_model import tag_constants\n",
    "\n",
    "# Set the learning phase to Test since the model is already trained.\n",
    "K.set_learning_phase(0)\n",
    "\n",
    "# Load the Keras model\n",
    "keras_model = tf.keras.models.load_model('model.h5')\n",
    "\n",
    "# Build the Protocol Buffer SavedModel at 'export_path'\n",
    "builder = saved_model_builder.SavedModelBuilder('protos')\n",
    "\n",
    "# Create prediction signature to be used by TensorFlow Serving Predict API\n",
    "signature = predict_signature_def(inputs={\"images\": keras_model.input},\n",
    "                                  outputs={\"scores\": keras_model.output})\n",
    "\n",
    "with tf.keras.backend.get_session() as sess:\n",
    "    # Save the meta graph and the variables\n",
    "    builder.add_meta_graph_and_variables(sess=sess, tags=[tag_constants.SERVING],\n",
    "                                     signature_def_map={\"predict\": signature})\n",
    "\n",
    "builder.save()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
