{
  "cells": [
    {
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true
      },
      "cell_type": "code",
      "source": "import pandas as pd\nimport numpy as np\nfrom PIL import Image\n\nimport tensorflow as tf\nimport keras\nfrom keras import backend as K\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom keras.optimizers import Adam\nfrom keras.metrics import categorical_crossentropy\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nimport os\nimport time\nfrom IPython import display\n\nfrom sklearn.model_selection import train_test_split\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\nos.listdir('../input')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
        "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
        "trusted": true
      },
      "cell_type": "code",
      "source": "# read metadata\n\ndf_meta = pd.read_csv('../input/metadata27368.csv')\n#df_meta.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "7feee9d8b646176e171ff660658bd4513d159eae"
      },
      "cell_type": "code",
      "source": "# import flavours\n\ndf_flav = pd.read_csv('../input/flavours.csv')\ndf_flav.head()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "8792082d62b7532d478eee5b9527320070d302e8"
      },
      "cell_type": "code",
      "source": "# import faulty images indexes\n\nfile = open(\"../input/faulty_images.txt\", \"r\")\nfaul_id = []\nfor val in file.read().split():\n    faul_id.append(int(val)-1)\nfile.close()\nprint(len(faul_id),\"faulty images\")",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d6743b0095fe215efd73add1864d7ec895f9ca7d"
      },
      "cell_type": "code",
      "source": "# get the unlabeled images indexes\n\n# search for nan values in flavours df\nnan_bool = df_flav.isnull().any(axis=1)\nnan_id = np.where(nan_bool == True)[0]\nprint(len(nan_id),'unlabeled images')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "45cd277b706880eb1dd6ba14e6281b9f450fa91e"
      },
      "cell_type": "code",
      "source": "# remove faulty images from unlabeles\n\nf_nan_id = []\nfor i in faul_id:\n    t = np.where(nan_id == i)[0]\n    if len(t) != 0:\n        f_nan_id.append(t)\nnan_id = np.delete(nan_id, f_nan_id)\n\nprint(len(nan_id),'true unlabeled images')",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "32604602add7aa1d8f3065a4203fc10783aa5cde"
      },
      "cell_type": "code",
      "source": "# Good images !\n\ngood_id = np.arange(27638)\ngood_id = np.delete(good_id, np.union1d(faul_id, nan_id))\n\nprint(len(good_id))\n\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ae6a42daad5f12176c62fddacb4de63535c1afc6"
      },
      "cell_type": "code",
      "source": "# Train and validation split \ntrain_id, val_id, _, _ = train_test_split(good_id, good_id, test_size=0.2)\n\nprint('Train', len(train_id))\nprint('Val',len(val_id))",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "c758cde5273739a57fd82c9432c543846afd79be"
      },
      "cell_type": "code",
      "source": "# creat a useful dictionary structures\n\npartition = {}\npartition['train'] = train_id\npartition['validation'] = val_id\npartition['faulty'] = faul_id\npartition['no_label'] = nan_id\n\nlabels = {}\nname = {}\nimage = {}\nfor i in range(27638):\n    \n    labels[str(i)] = df_flav.iloc[[i]].values[0] # every label is a panda df rows\n    \n    name[str(i)] = df_meta['name'][i]\n    \n    im = str(i+1)\n    while len(im) < 5 :\n        im = '0' + im\n    im ='img' + im + '.jpg'\n    image[str(i)] = im",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "d48fb5e59fa7d8a342f3e3ce4607f9daead7946b"
      },
      "cell_type": "code",
      "source": "# test dictionaries\n\nprint(labels['1'])\n\nprint(name['1'])\n\nprint(image['1'])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "660a89aae44101f2b53d9bbce0873fe1843c21f9"
      },
      "cell_type": "code",
      "source": "class DataGenerator(keras.utils.Sequence):\n    'Generates data for Keras'\n    def __init__(self, list_IDs, labels, batch_size=32, dim=(224, 224), n_channels=3,\n                 n_classes=6, shuffle=True):\n        'Initialization'\n        self.dim = dim\n        self.batch_size = batch_size\n        self.labels = labels\n        self.list_IDs = list_IDs\n        self.n_channels = n_channels\n        self.n_classes = n_classes\n        self.shuffle = shuffle\n        self.on_epoch_end()\n\n    def __len__(self):\n        'Denotes the number of batches per epoch'\n        return int(np.floor(len(self.list_IDs) / self.batch_size))\n\n    def __getitem__(self, index):\n        'Generate one batch of data'\n        # Generate indexes of the batch\n        indexes = self.indexes[index*self.batch_size:(index+1)*self.batch_size]\n\n        # Find list of IDs\n        list_IDs_temp = [self.list_IDs[k] for k in indexes]\n\n        # Generate data\n        X, y = self.__data_generation(list_IDs_temp)\n\n        return X, y\n\n    def on_epoch_end(self):\n        'Updates indexes after each epoch'\n        self.indexes = np.arange(len(self.list_IDs))\n        if self.shuffle == True:\n            np.random.shuffle(self.indexes)\n\n    def __data_generation(self, list_IDs_temp):\n        'Generates data containing batch_size samples' # X : (n_samples, *dim, n_channels)\n        # Initialization\n        X = np.empty((self.batch_size, *self.dim, self.n_channels))\n        y = np.empty((self.batch_size, self.n_classes))\n\n        # Generate data\n        for i, ID in enumerate(list_IDs_temp):\n            # Store sample\n            im = Image.open(\"../input/images27638/images27638/\"+image[str(ID)])\n            im = im.resize((224, 224), Image.ANTIALIAS)\n            im.load()\n            X[i,] = np.asarray(im, dtype=np.uint8 )\n\n            # Store target labels\n            y[i] = self.labels[str(ID)]\n\n        return X, y",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "1d3568e27131c55855767c858f473200c70a77a0"
      },
      "cell_type": "code",
      "source": "# create a copy of a mobilenet model\n\nmobile = keras.applications.mobilenet.MobileNet()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "503070dad3ce848542e8e4d3c9630f0b6027ae1b"
      },
      "cell_type": "code",
      "source": "mobile.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "9e6c05b039ec29a72fb75b128f653ddf711c1134"
      },
      "cell_type": "code",
      "source": "# CREATE THE MODEL ARCHITECTURE\n\n# Exclude the last 5 layers of the above model.\n# This will include all layers up to and including global_average_pooling2d_1\nx = mobile.layers[-6].output\n\n# Create a new dense layer for predictions\n# 7 corresponds to the number of classes\nx = Dropout(0.25)(x)\nx = Dense(512, activation='relu')(x)\nx = BatchNormalization()(x)\n\nx = Dropout(0.25)(x)\nx = Dense(256, activation='sigmoid')(x)\nx = BatchNormalization()(x)\n\nx = Dropout(0.25)(x)\npredictions = Dense(6, activation='linear')(x)\n\n# inputs=mobile.input selects the input layer, outputs=predictions refers to the\n# dense layer we created above.\n\nmodel = Model(inputs=mobile.input, outputs=predictions)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e81f62845e71041962f99d0fd0d398abb42f0634"
      },
      "cell_type": "code",
      "source": "model.summary()",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "ba8ed85e395b76e34faaae55d43de00428e6df26"
      },
      "cell_type": "code",
      "source": "# We need to choose how many layers we actually want to be trained.\n\n# Here we are freezing the weights of all layers except the\n# last 23 layers in the new model.\n# The last 23 layers of the model will be trained.\n\nfor layer in model.layers[:-23]:\n    layer.trainable = False",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "0620d56920cd01cbe904a4b5cb80ac4a0619fa25"
      },
      "cell_type": "code",
      "source": "# loss\ndef mse(y_true, y_pred):\n    \n    return K.mean(K.square(y_pred - y_true), axis=-1)\n\n# coefficient of determination (R^2) for regression\ndef r_square(y_true, y_pred):\n    SS_res =  K.sum(K.square(y_true - y_pred)) \n    SS_tot = K.sum(K.square(y_true - K.mean(y_true))) \n    return (1 - SS_res/(SS_tot + K.epsilon()))\n\n\nmodel.compile(Adam(lr=0.01), loss=mse, \n              metrics=[r_square])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "e3e84e000fe5a7127ece2e47405d4d58e01b2f58"
      },
      "cell_type": "code",
      "source": "def train_network(network, training_generator, validation_generator, n_epoch, network_filepath):\n\n\n    \n    # lists where we will be storing values during training, for visualization purposes\n    tra_losses = [] # list for training loss\n    tra_accs = [] # list for training accuracy\n    val_losses = [] # list for validation loss\n    val_accs = [] # list for validation accuracy\n\n    # we want to save the parameters that give the best performance on the validation set\n    # therefore, we store the best validation accuracy, and save the parameters to disk\n    best_validation_accuracy = 0 # best validation accuracy\n\n    for epoch in range(n_epoch):\n        st = time.time()\n\n        # Train your network\n        results = network.fit_generator(training_generator)\n\n        # Get training loss and accuracy\n        training_loss = results.history['loss']\n        training_accuracy = results.history['r_square']\n\n        # Add to list\n        tra_losses.append(training_loss)\n        tra_accs.append(training_accuracy)\n\n        # Evaluate performance (loss and accuracy) on validation set\n        scores = network.evaluate_generator(validation_generator)     \n        validation_loss = scores[0]\n        validation_accuracy = scores[1]\n\n        # Add to list\n        val_losses.append(validation_loss)\n        val_accs.append(validation_accuracy)\n\n        # (Possibly) update best validation accuracy and save the network\n        if validation_accuracy > best_validation_accuracy:\n            best_validation_accuracy = validation_accuracy\n            network.save(network_filepath)\n\n        # Visualization of the learning curves\n        fig = plt.figure(figsize=(10, 5))\n        tra_loss_plt, = plt.plot(range(len(tra_losses)), tra_losses, 'b')\n        tra_accs_plt, = plt.plot(range(len(tra_accs)), tra_accs, 'c')\n        val_loss_plt, = plt.plot(range(len(val_losses)), val_losses, 'g')\n        val_acc_plt, = plt.plot(range(len(val_accs)), val_accs, 'r')\n        plt.xlabel('epoch')\n        plt.ylabel('loss')\n        plt.legend([tra_loss_plt, tra_accs_plt, val_loss_plt, val_acc_plt], \n                  ['training loss', 'training accuracy', 'validation loss', 'validation accuracy'],\n                  loc='center left', bbox_to_anchor=(1, 0.5))\n        plt.title('Best validation accuracy = {:.2f}%'.format(100. * best_validation_accuracy))\n        display.clear_output(wait=True)\n        display.display(plt.gcf())\n        time.sleep(.2)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "5e200f091ada123d8d007ecf235d6b09c20c30e1"
      },
      "cell_type": "code",
      "source": "# Define batch size.\nbatch_size = 64\nn_epoch = 25\nval_size = len(val_id)\n\n\n# Parameters\nparams = {'dim': (224, 224),\n          'batch_size': batch_size,\n          'n_classes': 6,\n          'shuffle': True}\n\n# Generators\ntraining_generator = DataGenerator(partition['train'], labels, **params)\nvalidation_generator = DataGenerator(partition['validation'], labels, **params)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "64d275c911d98b86db43fa1f130d1740c4bd0d38"
      },
      "cell_type": "code",
      "source": "file_dir = '../working//Model'\nnetwork_filepath = os.path.join(file_dir, 'best_model.h5')\nos.mkdir(file_dir)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "878cdfe0984dde8bcede34ef8e0b284c64181359",
        "scrolled": true
      },
      "cell_type": "code",
      "source": "train_network(model, training_generator, validation_generator, n_epoch, network_filepath)",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "87f8c7f25bb7ce648f6c05217ef691fc9b813df7"
      },
      "cell_type": "code",
      "source": "# Re-load the model found for the best accuracy; load_model function takes care of compiling again the function \nbest_network=keras.models.load_model(network_filepath)\n",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "adc4ad87e09731c171ed008dea87c1d36043d5be"
      },
      "cell_type": "code",
      "source": "test = str(val_id[123])\n\nprint(name[test])\nprint(labels[test])\nim = Image.open(\"../input/images27638/images27638/\"+image[test])\nim = im.resize((224, 224), Image.ANTIALIAS)\nim.load()\nim = np.asarray(im, dtype=np.uint8 )\nim = np.expand_dims(im, axis=0)\n\n\npred = model.predict(im)\nprint('Prediction',pred[0])",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "trusted": true,
        "_uuid": "b1b73fd816551df269813928343ae30db02ef907"
      },
      "cell_type": "code",
      "source": "",
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.6.6",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 1
}